{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier import Loader \n",
    "from classifier import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusarual@GU.GU.SE/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  label\n",
      "0                                                          1\n",
      "1    WASHINGTON (Reuters) - A Super PAC backing Re...      0\n",
      "2   Hillary Clinton has turned down repeated reque...      1\n",
      "3   WASHINGTON (Reuters) - The Trump administratio...      0\n",
      "4   The anti-establishment tide is rolling across ...      1\n",
      "5   The following statementsÂ were posted to the ve...      0\n",
      "6   Donald Trump is attacking Obamacare instead of...      1\n",
      "7                                                          1\n",
      "8   BEIJING (Reuters) - China on Tuesday rejected ...      0\n",
      "9   FRANCE HAS THE BLOOD OF THIS PRIEST ON THIER H...      1\n",
      "10  Ben Carson s professional but hard hitting app...      1\n",
      "11  JAKARTA/CANDI DASA, Indonesia (Reuters) - Bali...      0\n",
      "12  WASHINGTON (Reuters) - Legislation with bipart...      0\n",
      "13  PRAGUE (Reuters) - Czech President Milos Zeman...      0\n",
      "14  TUNIS (Reuters) - Turkish President Tayyip Erd...      0\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusarual@GU.GU.SE/Machine-learning1/Project/classifier/data_loader.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "data= Loader()\n",
    "train,val,test= data.batcher()\n",
    "\n",
    "\n",
    "\n",
    "l =  data.y.label.to_frame().reset_index(drop=True)\n",
    "m = data.x.to_frame().reset_index(drop=True)\n",
    "\n",
    "df = pd.concat([m,l],axis=1)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(df.head(15))# 0 is true, 1 is fake \n",
    "\n",
    "print(\"done\")\n",
    "# the options given in the warnning message don't work. Only pad_to_max_length did padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44898 44898\n"
     ]
    }
   ],
   "source": [
    "print(len(df.label),len(df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/004 | Batch 001/1965 | Average Loss in last 1 iterations: 0.6868\n",
      "Epoch: 001/004 | Batch 026/1965 | Average Loss in last 25 iterations: 0.2557\n",
      "Epoch: 001/004 | Batch 051/1965 | Average Loss in last 25 iterations: 0.0446\n",
      "Epoch: 001/004 | Batch 076/1965 | Average Loss in last 25 iterations: 0.0189\n",
      "Epoch: 001/004 | Batch 101/1965 | Average Loss in last 25 iterations: 0.0383\n",
      "Epoch: 001/004 | Batch 126/1965 | Average Loss in last 25 iterations: 0.2240\n",
      "Epoch: 001/004 | Batch 151/1965 | Average Loss in last 25 iterations: 0.0610\n",
      "Epoch: 001/004 | Batch 176/1965 | Average Loss in last 25 iterations: 0.0482\n",
      "Epoch: 001/004 | Batch 201/1965 | Average Loss in last 25 iterations: 0.0025\n",
      "Epoch: 001/004 | Batch 226/1965 | Average Loss in last 25 iterations: 0.0586\n",
      "Epoch: 001/004 | Batch 251/1965 | Average Loss in last 25 iterations: 0.0209\n",
      "Epoch: 001/004 | Batch 276/1965 | Average Loss in last 25 iterations: 0.0720\n",
      "Epoch: 001/004 | Batch 301/1965 | Average Loss in last 25 iterations: 0.0002\n",
      "Epoch: 001/004 | Batch 326/1965 | Average Loss in last 25 iterations: 0.0270\n",
      "Epoch: 001/004 | Batch 351/1965 | Average Loss in last 25 iterations: 0.0275\n",
      "Epoch: 001/004 | Batch 376/1965 | Average Loss in last 25 iterations: 0.0205\n",
      "Epoch: 001/004 | Batch 401/1965 | Average Loss in last 25 iterations: 0.0015\n",
      "Epoch: 001/004 | Batch 426/1965 | Average Loss in last 25 iterations: 0.0008\n",
      "Epoch: 001/004 | Batch 451/1965 | Average Loss in last 25 iterations: 0.0751\n",
      "Epoch: 001/004 | Batch 476/1965 | Average Loss in last 25 iterations: 0.0412\n",
      "Epoch: 001/004 | Batch 501/1965 | Average Loss in last 25 iterations: 0.0151\n",
      "Epoch: 001/004 | Batch 526/1965 | Average Loss in last 25 iterations: 0.0723\n",
      "Epoch: 001/004 | Batch 551/1965 | Average Loss in last 25 iterations: 0.0298\n",
      "Epoch: 001/004 | Batch 576/1965 | Average Loss in last 25 iterations: 0.0004\n",
      "Epoch: 001/004 | Batch 601/1965 | Average Loss in last 25 iterations: 0.0302\n",
      "Epoch: 001/004 | Batch 626/1965 | Average Loss in last 25 iterations: 0.0394\n",
      "Epoch: 001/004 | Batch 651/1965 | Average Loss in last 25 iterations: 0.0191\n",
      "Epoch: 001/004 | Batch 676/1965 | Average Loss in last 25 iterations: 0.0457\n",
      "Epoch: 001/004 | Batch 701/1965 | Average Loss in last 25 iterations: 0.0342\n",
      "Epoch: 001/004 | Batch 726/1965 | Average Loss in last 25 iterations: 0.0018\n",
      "Epoch: 001/004 | Batch 751/1965 | Average Loss in last 25 iterations: 0.0012\n",
      "Epoch: 001/004 | Batch 776/1965 | Average Loss in last 25 iterations: 0.0203\n",
      "Epoch: 001/004 | Batch 801/1965 | Average Loss in last 25 iterations: 0.0205\n",
      "Epoch: 001/004 | Batch 826/1965 | Average Loss in last 25 iterations: 0.0007\n",
      "Epoch: 001/004 | Batch 851/1965 | Average Loss in last 25 iterations: 0.0369\n",
      "Epoch: 001/004 | Batch 876/1965 | Average Loss in last 25 iterations: 0.0026\n",
      "Epoch: 001/004 | Batch 901/1965 | Average Loss in last 25 iterations: 0.0170\n",
      "Epoch: 001/004 | Batch 926/1965 | Average Loss in last 25 iterations: 0.0010\n",
      "Epoch: 001/004 | Batch 951/1965 | Average Loss in last 25 iterations: 0.0176\n",
      "Epoch: 001/004 | Batch 976/1965 | Average Loss in last 25 iterations: 0.0160\n",
      "Epoch: 001/004 | Batch 1001/1965 | Average Loss in last 25 iterations: 0.0239\n",
      "Epoch: 001/004 | Batch 1026/1965 | Average Loss in last 25 iterations: 0.0017\n",
      "Epoch: 001/004 | Batch 1051/1965 | Average Loss in last 25 iterations: 0.0178\n",
      "Epoch: 001/004 | Batch 1076/1965 | Average Loss in last 25 iterations: 0.0348\n",
      "Epoch: 001/004 | Batch 1101/1965 | Average Loss in last 25 iterations: 0.0012\n",
      "Epoch: 001/004 | Batch 1126/1965 | Average Loss in last 25 iterations: 0.0214\n",
      "Epoch: 001/004 | Batch 1151/1965 | Average Loss in last 25 iterations: 0.0209\n",
      "Epoch: 001/004 | Batch 1176/1965 | Average Loss in last 25 iterations: 0.0492\n",
      "Epoch: 001/004 | Batch 1201/1965 | Average Loss in last 25 iterations: 0.0184\n",
      "Epoch: 001/004 | Batch 1226/1965 | Average Loss in last 25 iterations: 0.0190\n",
      "Epoch: 001/004 | Batch 1251/1965 | Average Loss in last 25 iterations: 0.0175\n",
      "Epoch: 001/004 | Batch 1276/1965 | Average Loss in last 25 iterations: 0.0189\n",
      "Epoch: 001/004 | Batch 1301/1965 | Average Loss in last 25 iterations: 0.0194\n",
      "Epoch: 001/004 | Batch 1326/1965 | Average Loss in last 25 iterations: 0.0180\n",
      "Epoch: 001/004 | Batch 1351/1965 | Average Loss in last 25 iterations: 0.0337\n",
      "Epoch: 001/004 | Batch 1376/1965 | Average Loss in last 25 iterations: 0.0327\n",
      "Epoch: 001/004 | Batch 1401/1965 | Average Loss in last 25 iterations: 0.0182\n",
      "Epoch: 001/004 | Batch 1426/1965 | Average Loss in last 25 iterations: 0.0023\n",
      "Epoch: 001/004 | Batch 1451/1965 | Average Loss in last 25 iterations: 0.0508\n",
      "Epoch: 001/004 | Batch 1476/1965 | Average Loss in last 25 iterations: 0.0015\n",
      "Epoch: 001/004 | Batch 1501/1965 | Average Loss in last 25 iterations: 0.0009\n",
      "Epoch: 001/004 | Batch 1526/1965 | Average Loss in last 25 iterations: 0.0209\n",
      "Epoch: 001/004 | Batch 1551/1965 | Average Loss in last 25 iterations: 0.0215\n",
      "Epoch: 001/004 | Batch 1576/1965 | Average Loss in last 25 iterations: 0.0927\n",
      "Epoch: 001/004 | Batch 1601/1965 | Average Loss in last 25 iterations: 0.9553\n",
      "Epoch: 001/004 | Batch 1626/1965 | Average Loss in last 25 iterations: 0.0789\n",
      "Epoch: 001/004 | Batch 1651/1965 | Average Loss in last 25 iterations: 0.0181\n",
      "Epoch: 001/004 | Batch 1676/1965 | Average Loss in last 25 iterations: 0.0028\n",
      "Epoch: 001/004 | Batch 1701/1965 | Average Loss in last 25 iterations: 0.0180\n",
      "Epoch: 001/004 | Batch 1726/1965 | Average Loss in last 25 iterations: 0.0503\n",
      "Epoch: 001/004 | Batch 1751/1965 | Average Loss in last 25 iterations: 0.0473\n",
      "Epoch: 001/004 | Batch 1776/1965 | Average Loss in last 25 iterations: 0.0030\n",
      "Epoch: 001/004 | Batch 1801/1965 | Average Loss in last 25 iterations: 0.0021\n",
      "Epoch: 001/004 | Batch 1826/1965 | Average Loss in last 25 iterations: 0.0010\n",
      "Epoch: 001/004 | Batch 1851/1965 | Average Loss in last 25 iterations: 0.0580\n",
      "Epoch: 001/004 | Batch 1876/1965 | Average Loss in last 25 iterations: 0.0714\n",
      "Epoch: 001/004 | Batch 1901/1965 | Average Loss in last 25 iterations: 0.0182\n",
      "Epoch: 001/004 | Batch 1926/1965 | Average Loss in last 25 iterations: 0.0177\n",
      "Epoch: 001/004 | Batch 1951/1965 | Average Loss in last 25 iterations: 0.0179\n",
      "Epoch: 002/004 | Batch 001/1965 | Average Loss in last 1 iterations: 0.0015\n",
      "Epoch: 002/004 | Batch 026/1965 | Average Loss in last 25 iterations: 0.0179\n",
      "Epoch: 002/004 | Batch 051/1965 | Average Loss in last 25 iterations: 0.0017\n",
      "Epoch: 002/004 | Batch 076/1965 | Average Loss in last 25 iterations: 0.0365\n",
      "Epoch: 002/004 | Batch 101/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 002/004 | Batch 126/1965 | Average Loss in last 25 iterations: 0.0011\n",
      "Epoch: 002/004 | Batch 151/1965 | Average Loss in last 25 iterations: 0.0557\n",
      "Epoch: 002/004 | Batch 176/1965 | Average Loss in last 25 iterations: 0.0403\n",
      "Epoch: 002/004 | Batch 201/1965 | Average Loss in last 25 iterations: 1.8410\n",
      "Epoch: 002/004 | Batch 226/1965 | Average Loss in last 25 iterations: 3.0799\n",
      "Epoch: 002/004 | Batch 251/1965 | Average Loss in last 25 iterations: 3.3561\n",
      "Epoch: 002/004 | Batch 276/1965 | Average Loss in last 25 iterations: 2.7523\n",
      "Epoch: 002/004 | Batch 301/1965 | Average Loss in last 25 iterations: 2.4137\n",
      "Epoch: 002/004 | Batch 326/1965 | Average Loss in last 25 iterations: 1.9724\n",
      "Epoch: 002/004 | Batch 351/1965 | Average Loss in last 25 iterations: 1.4662\n",
      "Epoch: 002/004 | Batch 376/1965 | Average Loss in last 25 iterations: 0.7199\n",
      "Epoch: 002/004 | Batch 401/1965 | Average Loss in last 25 iterations: 0.6953\n",
      "Epoch: 002/004 | Batch 426/1965 | Average Loss in last 25 iterations: 0.3132\n",
      "Epoch: 002/004 | Batch 451/1965 | Average Loss in last 25 iterations: 0.0900\n",
      "Epoch: 002/004 | Batch 476/1965 | Average Loss in last 25 iterations: 0.0223\n",
      "Epoch: 002/004 | Batch 501/1965 | Average Loss in last 25 iterations: 0.0352\n",
      "Epoch: 002/004 | Batch 526/1965 | Average Loss in last 25 iterations: 0.0186\n",
      "Epoch: 002/004 | Batch 551/1965 | Average Loss in last 25 iterations: 0.0335\n",
      "Epoch: 002/004 | Batch 576/1965 | Average Loss in last 25 iterations: 0.0179\n",
      "Epoch: 002/004 | Batch 601/1965 | Average Loss in last 25 iterations: 0.0018\n",
      "Epoch: 002/004 | Batch 626/1965 | Average Loss in last 25 iterations: 0.0008\n",
      "Epoch: 002/004 | Batch 651/1965 | Average Loss in last 25 iterations: 0.0417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002/004 | Batch 676/1965 | Average Loss in last 25 iterations: 0.0238\n",
      "Epoch: 002/004 | Batch 701/1965 | Average Loss in last 25 iterations: 0.0007\n",
      "Epoch: 002/004 | Batch 726/1965 | Average Loss in last 25 iterations: 0.0004\n",
      "Epoch: 002/004 | Batch 751/1965 | Average Loss in last 25 iterations: 0.0241\n",
      "Epoch: 002/004 | Batch 776/1965 | Average Loss in last 25 iterations: 0.0004\n",
      "Epoch: 002/004 | Batch 801/1965 | Average Loss in last 25 iterations: 0.0201\n",
      "Epoch: 002/004 | Batch 826/1965 | Average Loss in last 25 iterations: 0.0201\n",
      "Epoch: 002/004 | Batch 851/1965 | Average Loss in last 25 iterations: 0.0216\n",
      "Epoch: 002/004 | Batch 876/1965 | Average Loss in last 25 iterations: 2.5808\n",
      "Epoch: 002/004 | Batch 901/1965 | Average Loss in last 25 iterations: 2.8818\n",
      "Epoch: 002/004 | Batch 926/1965 | Average Loss in last 25 iterations: 2.4075\n",
      "Epoch: 002/004 | Batch 951/1965 | Average Loss in last 25 iterations: 1.9317\n",
      "Epoch: 002/004 | Batch 976/1965 | Average Loss in last 25 iterations: 1.5880\n",
      "Epoch: 002/004 | Batch 1001/1965 | Average Loss in last 25 iterations: 1.1528\n",
      "Epoch: 002/004 | Batch 1026/1965 | Average Loss in last 25 iterations: 0.9451\n",
      "Epoch: 002/004 | Batch 1051/1965 | Average Loss in last 25 iterations: 0.8105\n",
      "Epoch: 002/004 | Batch 1076/1965 | Average Loss in last 25 iterations: 0.5758\n",
      "Epoch: 002/004 | Batch 1101/1965 | Average Loss in last 25 iterations: 0.2594\n",
      "Epoch: 002/004 | Batch 1126/1965 | Average Loss in last 25 iterations: 0.0739\n",
      "Epoch: 002/004 | Batch 1151/1965 | Average Loss in last 25 iterations: 0.0750\n",
      "Epoch: 002/004 | Batch 1176/1965 | Average Loss in last 25 iterations: 0.0152\n",
      "Epoch: 002/004 | Batch 1201/1965 | Average Loss in last 25 iterations: 0.0263\n",
      "Epoch: 002/004 | Batch 1226/1965 | Average Loss in last 25 iterations: 0.0170\n",
      "Epoch: 002/004 | Batch 1251/1965 | Average Loss in last 25 iterations: 0.0025\n",
      "Epoch: 002/004 | Batch 1276/1965 | Average Loss in last 25 iterations: 0.0369\n",
      "Epoch: 002/004 | Batch 1301/1965 | Average Loss in last 25 iterations: 0.0175\n",
      "Epoch: 002/004 | Batch 1326/1965 | Average Loss in last 25 iterations: 0.0339\n",
      "Epoch: 002/004 | Batch 1351/1965 | Average Loss in last 25 iterations: 0.0189\n",
      "Epoch: 002/004 | Batch 1376/1965 | Average Loss in last 25 iterations: 0.0185\n",
      "Epoch: 002/004 | Batch 1401/1965 | Average Loss in last 25 iterations: 0.0483\n",
      "Epoch: 002/004 | Batch 1426/1965 | Average Loss in last 25 iterations: 0.0160\n",
      "Epoch: 002/004 | Batch 1451/1965 | Average Loss in last 25 iterations: 0.0021\n",
      "Epoch: 002/004 | Batch 1476/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 002/004 | Batch 1501/1965 | Average Loss in last 25 iterations: 0.0006\n",
      "Epoch: 002/004 | Batch 1526/1965 | Average Loss in last 25 iterations: 0.0199\n",
      "Epoch: 002/004 | Batch 1551/1965 | Average Loss in last 25 iterations: 0.0433\n",
      "Epoch: 002/004 | Batch 1576/1965 | Average Loss in last 25 iterations: 0.0388\n",
      "Epoch: 002/004 | Batch 1601/1965 | Average Loss in last 25 iterations: 0.0038\n",
      "Epoch: 002/004 | Batch 1626/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 002/004 | Batch 1651/1965 | Average Loss in last 25 iterations: 0.0185\n",
      "Epoch: 002/004 | Batch 1676/1965 | Average Loss in last 25 iterations: 0.0552\n",
      "Epoch: 002/004 | Batch 1701/1965 | Average Loss in last 25 iterations: 0.0372\n",
      "Epoch: 002/004 | Batch 1726/1965 | Average Loss in last 25 iterations: 0.0171\n",
      "Epoch: 002/004 | Batch 1751/1965 | Average Loss in last 25 iterations: 0.0502\n",
      "Epoch: 002/004 | Batch 1776/1965 | Average Loss in last 25 iterations: 0.0187\n",
      "Epoch: 002/004 | Batch 1801/1965 | Average Loss in last 25 iterations: 0.0211\n",
      "Epoch: 002/004 | Batch 1826/1965 | Average Loss in last 25 iterations: 0.0498\n",
      "Epoch: 002/004 | Batch 1851/1965 | Average Loss in last 25 iterations: 0.0181\n",
      "Epoch: 002/004 | Batch 1876/1965 | Average Loss in last 25 iterations: 0.0020\n",
      "Epoch: 002/004 | Batch 1901/1965 | Average Loss in last 25 iterations: 0.0017\n",
      "Epoch: 002/004 | Batch 1926/1965 | Average Loss in last 25 iterations: 0.0010\n",
      "Epoch: 002/004 | Batch 1951/1965 | Average Loss in last 25 iterations: 0.0195\n",
      "Epoch: 003/004 | Batch 001/1965 | Average Loss in last 1 iterations: 0.0008\n",
      "Epoch: 003/004 | Batch 026/1965 | Average Loss in last 25 iterations: 0.0179\n",
      "Epoch: 003/004 | Batch 051/1965 | Average Loss in last 25 iterations: 0.0214\n",
      "Epoch: 003/004 | Batch 076/1965 | Average Loss in last 25 iterations: 0.0347\n",
      "Epoch: 003/004 | Batch 101/1965 | Average Loss in last 25 iterations: 0.0015\n",
      "Epoch: 003/004 | Batch 126/1965 | Average Loss in last 25 iterations: 0.0363\n",
      "Epoch: 003/004 | Batch 151/1965 | Average Loss in last 25 iterations: 0.0012\n",
      "Epoch: 003/004 | Batch 176/1965 | Average Loss in last 25 iterations: 0.0189\n",
      "Epoch: 003/004 | Batch 201/1965 | Average Loss in last 25 iterations: 0.0182\n",
      "Epoch: 003/004 | Batch 226/1965 | Average Loss in last 25 iterations: 0.0176\n",
      "Epoch: 003/004 | Batch 251/1965 | Average Loss in last 25 iterations: 0.0015\n",
      "Epoch: 003/004 | Batch 276/1965 | Average Loss in last 25 iterations: 0.0168\n",
      "Epoch: 003/004 | Batch 301/1965 | Average Loss in last 25 iterations: 0.0208\n",
      "Epoch: 003/004 | Batch 326/1965 | Average Loss in last 25 iterations: 0.0009\n",
      "Epoch: 003/004 | Batch 351/1965 | Average Loss in last 25 iterations: 0.0371\n",
      "Epoch: 003/004 | Batch 376/1965 | Average Loss in last 25 iterations: 0.0174\n",
      "Epoch: 003/004 | Batch 401/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 003/004 | Batch 426/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 003/004 | Batch 451/1965 | Average Loss in last 25 iterations: 0.0507\n",
      "Epoch: 003/004 | Batch 476/1965 | Average Loss in last 25 iterations: 0.0012\n",
      "Epoch: 003/004 | Batch 501/1965 | Average Loss in last 25 iterations: 0.0185\n",
      "Epoch: 003/004 | Batch 526/1965 | Average Loss in last 25 iterations: 0.0347\n",
      "Epoch: 003/004 | Batch 551/1965 | Average Loss in last 25 iterations: 0.0180\n",
      "Epoch: 003/004 | Batch 576/1965 | Average Loss in last 25 iterations: 0.0016\n",
      "Epoch: 003/004 | Batch 601/1965 | Average Loss in last 25 iterations: 0.0194\n",
      "Epoch: 003/004 | Batch 626/1965 | Average Loss in last 25 iterations: 0.0354\n",
      "Epoch: 003/004 | Batch 651/1965 | Average Loss in last 25 iterations: 0.0179\n",
      "Epoch: 003/004 | Batch 676/1965 | Average Loss in last 25 iterations: 0.0184\n",
      "Epoch: 003/004 | Batch 701/1965 | Average Loss in last 25 iterations: 0.0511\n",
      "Epoch: 003/004 | Batch 726/1965 | Average Loss in last 25 iterations: 0.0167\n",
      "Epoch: 003/004 | Batch 751/1965 | Average Loss in last 25 iterations: 0.0168\n",
      "Epoch: 003/004 | Batch 776/1965 | Average Loss in last 25 iterations: 0.0330\n",
      "Epoch: 003/004 | Batch 801/1965 | Average Loss in last 25 iterations: 0.0162\n",
      "Epoch: 003/004 | Batch 826/1965 | Average Loss in last 25 iterations: 0.0181\n",
      "Epoch: 003/004 | Batch 851/1965 | Average Loss in last 25 iterations: 0.0020\n",
      "Epoch: 003/004 | Batch 876/1965 | Average Loss in last 25 iterations: 0.0337\n",
      "Epoch: 003/004 | Batch 901/1965 | Average Loss in last 25 iterations: 0.0017\n",
      "Epoch: 003/004 | Batch 926/1965 | Average Loss in last 25 iterations: 0.0014\n",
      "Epoch: 003/004 | Batch 951/1965 | Average Loss in last 25 iterations: 0.0362\n",
      "Epoch: 003/004 | Batch 976/1965 | Average Loss in last 25 iterations: 0.0010\n",
      "Epoch: 003/004 | Batch 1001/1965 | Average Loss in last 25 iterations: 0.0678\n",
      "Epoch: 003/004 | Batch 1026/1965 | Average Loss in last 25 iterations: 0.0014\n",
      "Epoch: 003/004 | Batch 1051/1965 | Average Loss in last 25 iterations: 0.0014\n",
      "Epoch: 003/004 | Batch 1076/1965 | Average Loss in last 25 iterations: 0.0183\n",
      "Epoch: 003/004 | Batch 1101/1965 | Average Loss in last 25 iterations: 0.0009\n",
      "Epoch: 003/004 | Batch 1126/1965 | Average Loss in last 25 iterations: 0.0009\n",
      "Epoch: 003/004 | Batch 1151/1965 | Average Loss in last 25 iterations: 0.0199\n",
      "Epoch: 003/004 | Batch 1176/1965 | Average Loss in last 25 iterations: 0.0007\n",
      "Epoch: 003/004 | Batch 1201/1965 | Average Loss in last 25 iterations: 0.0005\n",
      "Epoch: 003/004 | Batch 1226/1965 | Average Loss in last 25 iterations: 0.0761\n",
      "Epoch: 003/004 | Batch 1251/1965 | Average Loss in last 25 iterations: 0.0006\n",
      "Epoch: 003/004 | Batch 1276/1965 | Average Loss in last 25 iterations: 0.0187\n",
      "Epoch: 003/004 | Batch 1301/1965 | Average Loss in last 25 iterations: 0.0007\n",
      "Epoch: 003/004 | Batch 1326/1965 | Average Loss in last 25 iterations: 0.0007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/004 | Batch 1351/1965 | Average Loss in last 25 iterations: 0.0175\n",
      "Epoch: 003/004 | Batch 1376/1965 | Average Loss in last 25 iterations: 0.0186\n",
      "Epoch: 003/004 | Batch 1401/1965 | Average Loss in last 25 iterations: 0.0233\n",
      "Epoch: 003/004 | Batch 1426/1965 | Average Loss in last 25 iterations: 0.0010\n",
      "Epoch: 003/004 | Batch 1451/1965 | Average Loss in last 25 iterations: 0.0199\n",
      "Epoch: 003/004 | Batch 1476/1965 | Average Loss in last 25 iterations: 0.0179\n",
      "Epoch: 003/004 | Batch 1501/1965 | Average Loss in last 25 iterations: 0.0366\n",
      "Epoch: 003/004 | Batch 1526/1965 | Average Loss in last 25 iterations: 0.0181\n",
      "Epoch: 003/004 | Batch 1551/1965 | Average Loss in last 25 iterations: 0.0347\n",
      "Epoch: 003/004 | Batch 1576/1965 | Average Loss in last 25 iterations: 0.0173\n",
      "Epoch: 003/004 | Batch 1601/1965 | Average Loss in last 25 iterations: 0.0160\n",
      "Epoch: 003/004 | Batch 1626/1965 | Average Loss in last 25 iterations: 0.0192\n",
      "Epoch: 003/004 | Batch 1651/1965 | Average Loss in last 25 iterations: 0.0347\n",
      "Epoch: 003/004 | Batch 1676/1965 | Average Loss in last 25 iterations: 0.0173\n",
      "Epoch: 003/004 | Batch 1701/1965 | Average Loss in last 25 iterations: 0.0339\n",
      "Epoch: 003/004 | Batch 1726/1965 | Average Loss in last 25 iterations: 0.0181\n",
      "Epoch: 003/004 | Batch 1751/1965 | Average Loss in last 25 iterations: 0.0021\n",
      "Epoch: 003/004 | Batch 1776/1965 | Average Loss in last 25 iterations: 0.0182\n",
      "Epoch: 003/004 | Batch 1801/1965 | Average Loss in last 25 iterations: 0.0015\n",
      "Epoch: 003/004 | Batch 1826/1965 | Average Loss in last 25 iterations: 0.0014\n",
      "Epoch: 003/004 | Batch 1851/1965 | Average Loss in last 25 iterations: 0.0725\n",
      "Epoch: 003/004 | Batch 1876/1965 | Average Loss in last 25 iterations: 0.0188\n",
      "Epoch: 003/004 | Batch 1901/1965 | Average Loss in last 25 iterations: 0.0336\n",
      "Epoch: 003/004 | Batch 1926/1965 | Average Loss in last 25 iterations: 0.0176\n",
      "Epoch: 003/004 | Batch 1951/1965 | Average Loss in last 25 iterations: 0.0174\n",
      "Epoch: 004/004 | Batch 001/1965 | Average Loss in last 1 iterations: 0.0017\n",
      "Epoch: 004/004 | Batch 026/1965 | Average Loss in last 25 iterations: 0.0176\n",
      "Epoch: 004/004 | Batch 051/1965 | Average Loss in last 25 iterations: 0.0016\n",
      "Epoch: 004/004 | Batch 076/1965 | Average Loss in last 25 iterations: 0.0168\n",
      "Epoch: 004/004 | Batch 101/1965 | Average Loss in last 25 iterations: 0.0357\n",
      "Epoch: 004/004 | Batch 126/1965 | Average Loss in last 25 iterations: 0.0889\n",
      "Epoch: 004/004 | Batch 151/1965 | Average Loss in last 25 iterations: 0.0948\n",
      "Epoch: 004/004 | Batch 176/1965 | Average Loss in last 25 iterations: 0.1611\n",
      "Epoch: 004/004 | Batch 201/1965 | Average Loss in last 25 iterations: 0.2692\n",
      "Epoch: 004/004 | Batch 226/1965 | Average Loss in last 25 iterations: 0.0794\n",
      "Epoch: 004/004 | Batch 251/1965 | Average Loss in last 25 iterations: 0.4266\n",
      "Epoch: 004/004 | Batch 276/1965 | Average Loss in last 25 iterations: 0.9234\n",
      "Epoch: 004/004 | Batch 301/1965 | Average Loss in last 25 iterations: 0.2847\n",
      "Epoch: 004/004 | Batch 326/1965 | Average Loss in last 25 iterations: 0.1615\n",
      "Epoch: 004/004 | Batch 351/1965 | Average Loss in last 25 iterations: 0.1327\n",
      "Epoch: 004/004 | Batch 376/1965 | Average Loss in last 25 iterations: 0.1215\n",
      "Epoch: 004/004 | Batch 401/1965 | Average Loss in last 25 iterations: 0.0340\n",
      "Epoch: 004/004 | Batch 426/1965 | Average Loss in last 25 iterations: 0.0096\n",
      "Epoch: 004/004 | Batch 451/1965 | Average Loss in last 25 iterations: 0.0214\n",
      "Epoch: 004/004 | Batch 476/1965 | Average Loss in last 25 iterations: 0.0034\n",
      "Epoch: 004/004 | Batch 501/1965 | Average Loss in last 25 iterations: 0.0025\n",
      "Epoch: 004/004 | Batch 526/1965 | Average Loss in last 25 iterations: 0.0020\n",
      "Epoch: 004/004 | Batch 551/1965 | Average Loss in last 25 iterations: 0.0016\n",
      "Epoch: 004/004 | Batch 576/1965 | Average Loss in last 25 iterations: 0.0015\n",
      "Epoch: 004/004 | Batch 601/1965 | Average Loss in last 25 iterations: 0.0347\n",
      "Epoch: 004/004 | Batch 626/1965 | Average Loss in last 25 iterations: 0.0171\n",
      "Epoch: 004/004 | Batch 651/1965 | Average Loss in last 25 iterations: 0.0174\n",
      "Epoch: 004/004 | Batch 676/1965 | Average Loss in last 25 iterations: 0.0474\n",
      "Epoch: 004/004 | Batch 701/1965 | Average Loss in last 25 iterations: 0.0015\n",
      "Epoch: 004/004 | Batch 726/1965 | Average Loss in last 25 iterations: 0.0192\n",
      "Epoch: 004/004 | Batch 751/1965 | Average Loss in last 25 iterations: 0.0714\n",
      "Epoch: 004/004 | Batch 776/1965 | Average Loss in last 25 iterations: 0.0189\n",
      "Epoch: 004/004 | Batch 801/1965 | Average Loss in last 25 iterations: 0.0015\n",
      "Epoch: 004/004 | Batch 826/1965 | Average Loss in last 25 iterations: 0.0168\n",
      "Epoch: 004/004 | Batch 851/1965 | Average Loss in last 25 iterations: 0.0346\n",
      "Epoch: 004/004 | Batch 876/1965 | Average Loss in last 25 iterations: 0.0202\n",
      "Epoch: 004/004 | Batch 901/1965 | Average Loss in last 25 iterations: 0.0313\n",
      "Epoch: 004/004 | Batch 926/1965 | Average Loss in last 25 iterations: 0.0172\n",
      "Epoch: 004/004 | Batch 951/1965 | Average Loss in last 25 iterations: 0.0016\n",
      "Epoch: 004/004 | Batch 976/1965 | Average Loss in last 25 iterations: 0.0192\n",
      "Epoch: 004/004 | Batch 1001/1965 | Average Loss in last 25 iterations: 0.0015\n",
      "Epoch: 004/004 | Batch 1026/1965 | Average Loss in last 25 iterations: 0.0376\n",
      "Epoch: 004/004 | Batch 1051/1965 | Average Loss in last 25 iterations: 0.0167\n",
      "Epoch: 004/004 | Batch 1076/1965 | Average Loss in last 25 iterations: 0.0014\n",
      "Epoch: 004/004 | Batch 1101/1965 | Average Loss in last 25 iterations: 0.0200\n",
      "Epoch: 004/004 | Batch 1126/1965 | Average Loss in last 25 iterations: 0.0014\n",
      "Epoch: 004/004 | Batch 1151/1965 | Average Loss in last 25 iterations: 0.0179\n",
      "Epoch: 004/004 | Batch 1176/1965 | Average Loss in last 25 iterations: 0.0184\n",
      "Epoch: 004/004 | Batch 1201/1965 | Average Loss in last 25 iterations: 0.0326\n",
      "Epoch: 004/004 | Batch 1226/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 004/004 | Batch 1251/1965 | Average Loss in last 25 iterations: 0.0179\n",
      "Epoch: 004/004 | Batch 1276/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 004/004 | Batch 1301/1965 | Average Loss in last 25 iterations: 0.0012\n",
      "Epoch: 004/004 | Batch 1326/1965 | Average Loss in last 25 iterations: 0.0205\n",
      "Epoch: 004/004 | Batch 1351/1965 | Average Loss in last 25 iterations: 0.0011\n",
      "Epoch: 004/004 | Batch 1376/1965 | Average Loss in last 25 iterations: 0.0373\n",
      "Epoch: 004/004 | Batch 1401/1965 | Average Loss in last 25 iterations: 0.0186\n",
      "Epoch: 004/004 | Batch 1426/1965 | Average Loss in last 25 iterations: 0.0188\n",
      "Epoch: 004/004 | Batch 1451/1965 | Average Loss in last 25 iterations: 0.0352\n",
      "Epoch: 004/004 | Batch 1476/1965 | Average Loss in last 25 iterations: 0.0188\n",
      "Epoch: 004/004 | Batch 1501/1965 | Average Loss in last 25 iterations: 0.0180\n",
      "Epoch: 004/004 | Batch 1526/1965 | Average Loss in last 25 iterations: 0.0356\n",
      "Epoch: 004/004 | Batch 1551/1965 | Average Loss in last 25 iterations: 0.0012\n",
      "Epoch: 004/004 | Batch 1576/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 004/004 | Batch 1601/1965 | Average Loss in last 25 iterations: 0.0347\n",
      "Epoch: 004/004 | Batch 1626/1965 | Average Loss in last 25 iterations: 0.0348\n",
      "Epoch: 004/004 | Batch 1651/1965 | Average Loss in last 25 iterations: 0.0177\n",
      "Epoch: 004/004 | Batch 1676/1965 | Average Loss in last 25 iterations: 0.0361\n",
      "Epoch: 004/004 | Batch 1701/1965 | Average Loss in last 25 iterations: 0.0187\n",
      "Epoch: 004/004 | Batch 1726/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 004/004 | Batch 1751/1965 | Average Loss in last 25 iterations: 0.0185\n",
      "Epoch: 004/004 | Batch 1776/1965 | Average Loss in last 25 iterations: 0.0182\n",
      "Epoch: 004/004 | Batch 1801/1965 | Average Loss in last 25 iterations: 0.0322\n",
      "Epoch: 004/004 | Batch 1826/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 004/004 | Batch 1851/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 004/004 | Batch 1876/1965 | Average Loss in last 25 iterations: 0.0345\n",
      "Epoch: 004/004 | Batch 1901/1965 | Average Loss in last 25 iterations: 0.0013\n",
      "Epoch: 004/004 | Batch 1926/1965 | Average Loss in last 25 iterations: 0.0187\n",
      "Epoch: 004/004 | Batch 1951/1965 | Average Loss in last 25 iterations: 0.0515\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "\n",
    "\n",
    "model = Model(device)\n",
    "epochs=4\n",
    "\n",
    "training_stats = model.Trainer(train,val,epochs)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "# Normal warning message as the model is expected to be fine-tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_Loss</th>\n",
       "      <th>val_Accur</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.446</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_loss  val_Loss  val_Accur\n",
       "epoch                                 \n",
       "0           0.044      0.01      0.999\n",
       "1           0.446      0.01      0.999\n",
       "2           0.019      0.01      0.999\n",
       "3           0.050      0.01      0.999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "df_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{' Loss': 0.016935177453134036, ' Accur': 0.9976645907473309}]\n",
      "[[4259    7]\n",
      " [  14 4699]]\n"
     ]
    }
   ],
   "source": [
    "test_stat,predictions,labels = model.Test(test)\n",
    "\n",
    "print(test_stat)\n",
    "c_metric = metrics.confusion_matrix(predictions,labels)\n",
    "print(c_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "labels = np.array([['4259','7'],['14','4699']])\n",
    "sns.heatmap(c_metric, annot=labels, fmt = '')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
